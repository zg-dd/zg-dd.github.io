<!doctype html><html lang=en dir=ltr><head><meta charset=utf-8><meta name=viewport content='width=device-width,initial-scale=1'><meta name=description content="Core Concepts Explained Simply\rWhat is topK?\rtopK means &ldquo;the top K most similar results&rdquo;.\nExample: topK=5 means return the 5 most similar data entries Example: topK=100 means return the 100 most similar data entries Analogy: Like searching for &ldquo;phones&rdquo; on Taobao, you only look at the top 10 products - that 10 is the topK What is Recall Rate?\rRecall rate is a metric measuring search accuracy, indicating &ldquo;how many truly similar results were found&rdquo;.\n"><title>Milvus Index Detailed Guide - Vector Search Performance Optimization</title><link rel=canonical href=https://zg-dd.github.io/en/stu/milvus-index-explained/><link rel=stylesheet href=/scss/style.min.ff560f1cad7a040e336f2dd6880b9ee1ad344d6e3d8f5411e5a1aaa8b23bbdd0.css><meta property='og:title' content="Milvus Index Detailed Guide - Vector Search Performance Optimization"><meta property='og:description' content="Core Concepts Explained Simply\rWhat is topK?\rtopK means &ldquo;the top K most similar results&rdquo;.\nExample: topK=5 means return the 5 most similar data entries Example: topK=100 means return the 100 most similar data entries Analogy: Like searching for &ldquo;phones&rdquo; on Taobao, you only look at the top 10 products - that 10 is the topK What is Recall Rate?\rRecall rate is a metric measuring search accuracy, indicating &ldquo;how many truly similar results were found&rdquo;.\n"><meta property='og:url' content='https://zg-dd.github.io/en/stu/milvus-index-explained/'><meta property='og:site_name' content='Â∞èÂ∑∑ Blog'><meta property='og:type' content='article'><meta property='article:section' content='Stu'><meta property='article:tag' content='Milvus'><meta property='article:tag' content='Index'><meta property='article:published_time' content='2026-02-27T19:11:18+08:00'><meta property='article:modified_time' content='2026-02-27T19:11:18+08:00'><meta name=twitter:title content="Milvus Index Detailed Guide - Vector Search Performance Optimization"><meta name=twitter:description content="Core Concepts Explained Simply\rWhat is topK?\rtopK means &ldquo;the top K most similar results&rdquo;.\nExample: topK=5 means return the 5 most similar data entries Example: topK=100 means return the 100 most similar data entries Analogy: Like searching for &ldquo;phones&rdquo; on Taobao, you only look at the top 10 products - that 10 is the topK What is Recall Rate?\rRecall rate is a metric measuring search accuracy, indicating &ldquo;how many truly similar results were found&rdquo;.\n"></head><body class=article-page><script>(function(){const e="StackColorScheme";localStorage.getItem(e)||localStorage.setItem(e,"auto")})()</script><script>(function(){const t="StackColorScheme",e=localStorage.getItem(t),n=window.matchMedia("(prefers-color-scheme: dark)").matches===!0;e=="dark"||e==="auto"&&n?document.documentElement.dataset.scheme="dark":document.documentElement.dataset.scheme="light"})()</script><div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky"><button class="hamburger hamburger--spin" type=button id=toggle-menu aria-label="Toggle Menu">
<span class=hamburger-box><span class=hamburger-inner></span></span></button><header><figure class=site-avatar><a href=/en/><img src=/img/avatar_hu_a0cc1cf57968b27d.png width=300 height=300 class=site-logo loading=lazy alt=Avatar>
</a><span class=emoji>üå±</span></figure><div class=site-meta><h1 class=site-name><a href=/en>Â∞èÂ∑∑ Blog</a></h1><h2 class=site-description>Recording is not to prove myself; rather, to make a habit of 'solving problems ‚Üí thinking ‚Üí sedimentation'.</h2></div></header><ol class=menu id=main-menu><li class=current><a href=/en/stu/><span>Study Notes</span></a></li><li><a href=/en/blog/><span>Essays</span></a></li><li><a href=/en/life/><span>Life</span></a></li><li><a href=/categories/><span>Categories</span></a></li><li><a href=/tags/><span>Tags</span></a></li><li class=menu-bottom-section><ol class=menu><li id=i18n-switch><svg class="icon icon-tabler icon-tabler-language" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M4 5h7"/><path d="M9 3v2c0 4.418-2.239 8-5 8"/><path d="M5 9c-.003 2.144 2.952 3.908 6.7 4"/><path d="M12 20l4-9 4 9"/><path d="M19.1 18h-6.2"/></svg>
<select name=language title=language onchange="window.location.href=this.selectedOptions[0].value"><option value=https://zg-dd.github.io/>ÁÆÄ‰Ωì‰∏≠Êñá</option><option value=https://zg-dd.github.io/en/ selected>English</option></select></li><li id=dark-mode-toggle><svg class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="8" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<svg class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="16" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<span>Dark Mode</span></li></ol></li></ol></aside><aside class="sidebar right-sidebar sticky"><section class="widget archives"><div class=widget-icon><svg class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg></div><h2 class="widget-title section-title">Table of contents</h2><div class=widget--toc><nav id=TableOfContents><ul><li><a href=#core-concepts-explained-simply>Core Concepts Explained Simply</a><ul><li><a href=#what-is-topk>What is topK?</a></li><li><a href=#what-is-recall-rate>What is Recall Rate?</a></li><li><a href=#what-is-filter-ratio>What is Filter Ratio?</a></li><li><a href=#what-is-capacity>What is Capacity?</a></li><li><a href=#what-is-autoindex>What is AutoIndex?</a></li></ul></li><li><a href=#why-indexing-is-needed>Why Indexing is Needed</a></li><li><a href=#supported-index-types>Supported Index Types</a></li><li><a href=#vector-index-internal-structure>Vector Index Internal Structure</a><ul><li><a href=#common-data-structures>Common Data Structures</a></li><li><a href=#quantization-methods>Quantization Methods</a></li><li><a href=#refiner>Refiner</a></li></ul></li><li><a href=#performance-trade-offs-and-recommended-usage-scenarios>Performance Trade-offs and Recommended Usage Scenarios</a><ul><li><a href=#capacity-recommendations>Capacity Recommendations</a></li><li><a href=#recall-rate-vs-filter-ratio>Recall Rate vs Filter Ratio</a></li><li><a href=#performance-decision-matrix-recommended-scenarios>Performance Decision Matrix (Recommended Scenarios)</a></li></ul></li><li><a href=#practical-application-scenarios>Practical Application Scenarios</a><ul><li><a href=#scenario-1-e-commerce-product-recommendation-system>Scenario 1: E-commerce Product Recommendation System</a></li><li><a href=#scenario-2-large-scale-image-search>Scenario 2: Large-scale Image Search</a></li><li><a href=#scenario-3-real-time-chatbot-semantic-search>Scenario 3: Real-time Chatbot (Semantic Search)</a></li><li><a href=#scenario-4-video-content-moderation>Scenario 4: Video Content Moderation</a></li><li><a href=#scenario-5-user-profile-matching-with-filter-conditions>Scenario 5: User Profile Matching (with filter conditions)</a></li><li><a href=#scenario-6-beginner-rapid-prototyping>Scenario 6: Beginner Rapid Prototyping</a></li></ul></li><li><a href=#memory-usage-estimation-examples-1-million-128-dimensional-vectors>Memory Usage Estimation Examples (1 million 128-dimensional vectors)</a><ul><li><a href=#ivf-series>IVF Series</a></li><li><a href=#hnsw-series>HNSW Series</a></li></ul></li><li><a href=#metric-types-detailed-explanation>Metric Types Detailed Explanation</a><ul><li><a href=#what-are-metric-types>What are Metric Types?</a></li><li><a href=#supported-metric-types-mapping-table>Supported Metric Types Mapping Table</a></li><li><a href=#metric-types-feature-comparison>Metric Types Feature Comparison</a></li><li><a href=#1-euclidean-distance-l2>1. Euclidean Distance (L2)</a></li><li><a href=#2-inner-product-ip>2. Inner Product (IP)</a></li><li><a href=#3-cosine-similarity>3. Cosine Similarity</a></li><li><a href=#4-jaccard-distance>4. Jaccard Distance</a></li><li><a href=#5-minhash-jaccard-mhjaccard>5. MinHash Jaccard (MHJACCARD)</a></li><li><a href=#6-hamming-distance>6. Hamming Distance</a></li><li><a href=#7-bm25-similarity>7. BM25 Similarity</a></li><li><a href=#how-to-choose-metric-types>How to Choose Metric Types?</a></li><li><a href=#usage-examples>Usage Examples</a></li><li><a href=#performance-comparison>Performance Comparison</a></li></ul></li><li><a href=#summary>Summary</a></li></ul></nav></div></section></aside><main class="main full-width"><article class=main-article><header class=article-header><div class=article-details><header class=article-category><a href=/en/categories/vector-database/>Vector Database</a></header><div class=article-title-wrapper><h2 class=article-title><a href=/en/stu/milvus-index-explained/>Milvus Index Detailed Guide - Vector Search Performance Optimization</a></h2></div><footer class=article-time><div><svg class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11.795 21H5a2 2 0 01-2-2V7a2 2 0 012-2h12a2 2 0 012 2v4"/><circle cx="18" cy="18" r="4"/><path d="M15 3v4"/><path d="M7 3v4"/><path d="M3 11h16"/><path d="M18 16.496V18l1 1"/></svg>
<time class=article-time--published datetime=2026-02-27T19:11:18+08:00>Feb 27, 2026</time></div><div><svg class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
<time class=article-time--reading>13 minute read</time></div></footer><footer class=article-translations><svg class="icon icon-tabler icon-tabler-language" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M4 5h7"/><path d="M9 3v2c0 4.418-2.239 8-5 8"/><path d="M5 9c-.003 2.144 2.952 3.908 6.7 4"/><path d="M12 20l4-9 4 9"/><path d="M19.1 18h-6.2"/></svg><div><a href=https://zg-dd.github.io/stu/milvus-index-explained/ class=link>ÁÆÄ‰Ωì‰∏≠Êñá</a></div></footer></div></header><section class=article-content><h2 id=core-concepts-explained-simply>Core Concepts Explained Simply</h2><h3 id=what-is-topk>What is topK?</h3><p><strong>topK</strong> means &ldquo;the top K most similar results&rdquo;.</p><ul><li>Example: topK=5 means return the 5 most similar data entries</li><li>Example: topK=100 means return the 100 most similar data entries</li><li><strong>Analogy</strong>: Like searching for &ldquo;phones&rdquo; on Taobao, you only look at the top 10 products - that 10 is the topK</li></ul><h3 id=what-is-recall-rate>What is Recall Rate?</h3><p><strong>Recall rate</strong> is a metric measuring search accuracy, indicating &ldquo;how many truly similar results were found&rdquo;.</p><ul><li><strong>Formula</strong>: Recall rate = Actual similar results found / Theoretically should find similar results</li><li><strong>Example</strong>: If among the truly 10 most similar results, your search found 9, the recall rate is 90%</li><li><strong>Analogy</strong>: Like an exam, full score 100 you got 95, recall rate is 95%</li><li><strong>Importance</strong>: Higher recall rate means more accurate search results, but potentially slower speed</li></ul><h3 id=what-is-filter-ratio>What is Filter Ratio?</h3><p><strong>Filter ratio</strong> refers to the proportion of remaining data after conditional filtering compared to total data.</p><ul><li><strong>Formula</strong>: Filter ratio = (Total data - Remaining data after filtering) / Total data</li><li><strong>Example</strong>: 1 million data entries, filtering condition is &ldquo;price > 1000&rdquo;, 50 thousand remain, filter ratio = (1M-50K)/1M = 95%</li><li><strong>Analogy</strong>: Like finding books in a library, first filter by &ldquo;Computer Science&rdquo; to eliminate 90% of books, filter ratio is 90%</li><li><strong>Impact</strong>: Higher filter ratio means less remaining data, brute force search might be faster</li></ul><h3 id=what-is-capacity>What is Capacity?</h3><p><strong>Capacity</strong> refers to your hardware resources (mainly memory) that can store how much data.</p><ul><li><strong>Memory capacity</strong>: How much RAM your server has available to store vector data</li><li><strong>Example</strong>: You have 16GB memory, vector data needs 64GB, so only 1/4 data can fit in memory</li><li><strong>Analogy</strong>: Like your phone storage space, too many photos need deletion or cloud storage</li><li><strong>Strategies</strong>:<ul><li>All data fits in memory ‚Üí Use in-memory index (fast)</li><li>Only part of data fits in memory ‚Üí Use disk index (slower but handles big data)</li><li>Severe memory shortage ‚Üí Use compressed index (trade precision for space)</li></ul></li></ul><h3 id=what-is-autoindex>What is AutoIndex?</h3><p><strong>AutoIndex</strong> is Milvus&rsquo;s intelligent index selection feature, automatically choosing the most suitable index type based on your data characteristics.</p><p><strong>Working principle</strong>:</p><ol><li>Analyze data scale, dimensions, distribution characteristics</li><li>Evaluate hardware resources (memory, CPU, GPU)</li><li>Automatically select optimal index type and parameters</li><li>Automatically build and optimize index</li></ol><p><strong>Applicable scenarios</strong>:</p><ul><li>Beginners unsure what index to choose</li><li>Complex data characteristics difficult to manually select</li><li>Rapid prototyping, don&rsquo;t want to spend time tuning</li></ul><p><strong>Usage</strong>:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># Vector field uses AutoIndex</span>
</span></span><span style=display:flex><span>index_params<span style=color:#f92672>.</span>add_index(
</span></span><span style=display:flex><span>    field_name<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;embedding&#34;</span>,
</span></span><span style=display:flex><span>    index_type<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;AUTOINDEX&#34;</span>,  <span style=color:#75715e># Automatically select index type</span>
</span></span><span style=display:flex><span>    metric_type<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;L2&#34;</span>
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Scalar fields can also use automatic indexing</span>
</span></span><span style=display:flex><span><span style=color:#75715e># Milvus will automatically choose INVERTED, BITMAP, or STL_SORT based on field type</span>
</span></span></code></pre></div><p><strong>Notes</strong>:</p><ul><li>AutoIndex suits most scenarios, but manual tuning recommended for extreme performance requirements</li><li>Different Milvus versions may have different AutoIndex strategies</li><li>Zilliz Cloud (managed service) uses AutoIndex by default</li></ul><h2 id=why-indexing-is-needed>Why Indexing is Needed</h2><p>Indexes are additional structures built on data, whose internal organization depends on the <strong>Approximate Nearest Neighbor Search algorithm</strong> (ANNS). The main purpose of indexes is to accelerate the search process, but comes with these costs:</p><ul><li>Additional preprocessing (building) time</li><li>Occupies more storage space</li><li>Consumes more RAM during queries</li><li>Usually slightly reduces <strong>recall rate</strong>, though impact is typically controllable</li></ul><p><strong>Analogy understanding</strong>: Indexes are like book tables of contents - though they take up a few pages, they let you quickly find desired chapters instead of flipping from beginning to end.</p><p>Therefore, choosing the right index requires balancing acceleration benefits against the above costs.</p><h2 id=supported-index-types>Supported Index Types</h2><p>Indexes in Milvus are defined for specific fields, with different field data types corresponding to different applicable indexes. Here&rsquo;s the officially supported mapping relationship (focus on vector indexes):</p><div class=table-wrapper><table><thead><tr><th>Field Data Type</th><th>Applicable Index Types</th></tr></thead><tbody><tr><td>FLOAT_VECTOR / FLOAT16_VECTOR / BFLOAT16_VECTOR / INT8_VECTOR</td><td>FLAT, IVF_FLAT, IVF_SQ8, IVF_PQ, IVF_RABITQ, HNSW, HNSW_SQ, HNSW_PQ, HNSW_PRQ, DISKANN, SCANN, AISAQ, GPU_CAGRA, GPU_IVF_FLAT, GPU_IVF_PQ, GPU_BRUTE_FORCE</td></tr><tr><td>BINARY_VECTOR</td><td>BIN_FLAT, BIN_IVF_FLAT, MINHASH_LSH</td></tr><tr><td>SPARSE_FLOAT_VECTOR</td><td>SPARSE_INVERTED_INDEX</td></tr><tr><td>VARCHAR</td><td>INVERTED (recommended), BITMAP, Trie</td></tr><tr><td>BOOL</td><td>BITMAP (recommended), INVERTED</td></tr><tr><td>INT8 / INT16 / INT32 / INT64</td><td>INVERTED, STL_SORT</td></tr><tr><td>FLOAT / DOUBLE</td><td>INVERTED</td></tr><tr><td>ARRAY (elements BOOL / INT8/16/32/64 / VARCHAR)</td><td>BITMAP (recommended)</td></tr><tr><td>ARRAY (elements BOOL / INT8/16/32/64 / FLOAT / DOUBLE / VARCHAR)</td><td>INVERTED</td></tr><tr><td>JSON</td><td>INVERTED</td></tr></tbody></table></div><p><strong>Scalar field recommendation</strong>: Prioritize index types marked &ldquo;recommended&rdquo; in the table. This article focuses on vector index selection.</p><h2 id=vector-index-internal-structure>Vector Index Internal Structure</h2><p>Milvus vector indexes typically consist of three core components:</p><ol><li><strong>Data Structure</strong> - For coarse-grained filtering</li><li><strong>Quantization</strong> (optional) - Compressed representation, reducing memory and computational overhead</li><li><strong>Refiner</strong> (optional) - Recalculate distances with higher precision for quantized candidate results, improving recall rate</li></ol><p>During index building, Milvus automatically determines an appropriate <strong>expansion rate</strong> based on selected data structure and quantization method. During queries, first retrieve <code>topK √ó expansion rate</code> candidate vectors, then refine through the refiner to return the most precise topK results.</p><h3 id=common-data-structures>Common Data Structures</h3><ul><li><p><strong>Inverted File (IVF)</strong><br>Clusters vectors into multiple buckets through centroids. During queries, only scan centroid-proximate buckets, dramatically reducing computation.<br><strong>Applicable scenarios</strong>: Large-scale datasets, high throughput requirements.</p></li><li><p><strong>Graph-based structures</strong><br>Represented by HNSW (Hierarchical Navigable Small World), builds hierarchical graph structure with each vector connecting to nearest neighbors. Querying descends from coarse upper layers gradually, achieving logarithmic time complexity.<br><strong>Applicable scenarios</strong>: High-dimensional data, low-latency queries.</p></li></ul><h3 id=quantization-methods>Quantization Methods</h3><ul><li><strong>Scalar Quantization (SQ)</strong> like SQ8: Compresses each dimension to 1 byte, memory reduced by ~75%, acceptable accuracy loss.</li><li><strong>Product Quantization (PQ)</strong>: Slices vectors into sub-vectors and encodes, can achieve 4-32x compression, slight recall rate decrease, suitable for memory-constrained environments.</li></ul><h3 id=refiner>Refiner</h3><p>Quantization is lossy. To ensure recall rate, the system generates more candidates; the refiner recalculates distances for these candidates using original precision (like FP32). This is crucial for distance-sensitive applications like semantic search and recommendation systems.</p><h2 id=performance-trade-offs-and-recommended-usage-scenarios>Performance Trade-offs and Recommended Usage Scenarios</h2><p>Choosing indexes requires comprehensive consideration of <strong>build time</strong>, <strong>query throughput (QPS)</strong>, <strong>recall rate</strong>, <strong>memory/disk usage</strong>.</p><h3 id=capacity-recommendations>Capacity Recommendations</h3><p><strong>Simple explanation</strong>: Based on your memory size, choose appropriate indexing strategy.</p><div class=table-wrapper><table><thead><tr><th>Memory situation</th><th>Recommended strategy</th><th>Explanation</th></tr></thead><tbody><tr><td>1/4 raw data fits in memory</td><td>DiskANN</td><td>Part data in memory, part on disk, stable latency</td></tr><tr><td>All data fits in memory</td><td>In-memory index (HNSW/IVF) + mmap</td><td>Fastest speed, recommended</td></tr><tr><td>Extremely memory constrained</td><td>Quantized index (IVF_PQ/IVF_SQ8) + mmap</td><td>Compress data, trade precision for space</td></tr><tr><td>Most data on disk</td><td>DiskANN</td><td>Disk-optimized, good latency performance</td></tr></tbody></table></div><p><strong>Real examples</strong>:</p><ul><li>You have 1 million 128-dimensional vectors, raw data ~500MB</li><li>If your server has 8GB memory ‚Üí Use HNSW (all data in memory)</li><li>If your server has only 2GB memory ‚Üí Use IVF_PQ (compress to under 100MB)</li><li>If you have 1 billion vectors (500GB) ‚Üí Use DiskANN (disk index)</li></ul><h3 id=recall-rate-vs-filter-ratio>Recall Rate vs Filter Ratio</h3><p><strong>Simple explanation</strong>: Based on strictness of your filtering conditions, choose appropriate index.</p><div class=table-wrapper><table><thead><tr><th>Filter ratio</th><th>Remaining data proportion</th><th>Recommended index</th><th>Reason</th></tr></thead><tbody><tr><td>&lt; 85%</td><td>Remaining > 15%</td><td>HNSW (graph index)</td><td>Data volume still large, need efficient indexing</td></tr><tr><td>85%-95%</td><td>Remaining 5%-15%</td><td>IVF series</td><td>Medium data volume, clustering index works well</td></tr><tr><td>> 98%</td><td>Remaining &lt; 2%</td><td>FLAT (brute force)</td><td>Very little data, direct calculation faster</td></tr></tbody></table></div><p><strong>Real examples</strong>:</p><ul><li>1 million data, filtering &ldquo;price > 1000&rdquo; leaves 500K (filter ratio 50%) ‚Üí Use HNSW</li><li>1 million data, filtering &ldquo;VIP users&rdquo; leaves 100K (filter ratio 90%) ‚Üí Use IVF_FLAT</li><li>1 million data, filtering &ldquo;registered today&rdquo; leaves 100 (filter ratio 99.99%) ‚Üí Use FLAT</li></ul><h3 id=performance-decision-matrix-recommended-scenarios>Performance Decision Matrix (Recommended Scenarios)</h3><div class=table-wrapper><table><thead><tr><th>Scenario Description</th><th>Recommended Index Type</th><th>Notes</th></tr></thead><tbody><tr><td>Raw data all fits in memory</td><td>HNSW, IVF + Refinement</td><td>HNSW suitable for small k / high recall</td></tr><tr><td>Raw data on SSD disk</td><td>DiskANN</td><td>Preferred for latency-sensitive queries</td></tr><tr><td>Raw data on disk, limited RAM</td><td>IVF_PQ / SQ + mmap</td><td>Balance memory and disk access</td></tr><tr><td>High filter ratio (>95%)</td><td>Brute-Force (FLAT)</td><td>Avoid index overhead</td></tr><tr><td>Large k (‚â•1% of dataset)</td><td>IVF</td><td>Clustering pruning reduces computation</td></tr><tr><td>Very high recall rate (>99%)</td><td>Brute-Force (FLAT) + GPU</td><td>‚Äî</td></tr></tbody></table></div><p><strong>General patterns</strong>:</p><ul><li>Graph indexes (HNSW etc.) usually have higher QPS than IVF variants.</li><li>IVF variants better suited for large topK (e.g. >2000).</li><li>PQ has better recall rate than SQ at same compression ratio, but SQ is faster.</li></ul><h2 id=practical-application-scenarios>Practical Application Scenarios</h2><h3 id=scenario-1-e-commerce-product-recommendation-system>Scenario 1: E-commerce Product Recommendation System</h3><p><strong>Data scale</strong>: 1 million products, each product 512-dimensional vector
<strong>Memory</strong>: 16GB
<strong>Requirements</strong>: Fast response (&lt; 50ms), recall rate > 95%
<strong>Recommended index</strong>: HNSW
<strong>Reason</strong>: Data can all fit in memory, HNSW provides optimal speed and recall balance</p><h3 id=scenario-2-large-scale-image-search>Scenario 2: Large-scale Image Search</h3><p><strong>Data scale</strong>: 100 million images, each 2048-dimensional vector
<strong>Memory</strong>: 64GB
<strong>Requirements</strong>: Support high concurrency, acceptable 100ms latency
<strong>Recommended index</strong>: DiskANN
<strong>Reason</strong>: Data volume too large for all-in-memory, DiskANN optimized for disk</p><h3 id=scenario-3-real-time-chatbot-semantic-search>Scenario 3: Real-time Chatbot (Semantic Search)</h3><p><strong>Data scale</strong>: 100K knowledge base entries, each 768-dimensional vector
<strong>Memory</strong>: 8GB
<strong>Requirements</strong>: Very high recall rate (> 99%), latency &lt; 20ms
<strong>Recommended index</strong>: FLAT (brute force)
<strong>Reason</strong>: Small data volume, brute force search fast with 100% recall rate</p><h3 id=scenario-4-video-content-moderation>Scenario 4: Video Content Moderation</h3><p><strong>Data scale</strong>: 50 million video frames, each 256-dimensional vector
<strong>Memory</strong>: 32GB (constrained)
<strong>Requirements</strong>: Cost priority, acceptable 90% recall rate
<strong>Recommended index</strong>: IVF_PQ
<strong>Reason</strong>: Memory constrained, PQ compression saves 90% memory</p><h3 id=scenario-5-user-profile-matching-with-filter-conditions>Scenario 5: User Profile Matching (with filter conditions)</h3><p><strong>Data scale</strong>: 5 million users, each user 128-dimensional vector
<strong>Memory</strong>: 16GB
<strong>Requirements</strong>: Frequently need to filter by &ldquo;age, gender, region&rdquo; then search
<strong>Recommended index</strong>:</p><ul><li>Vector field: HNSW</li><li>Scalar fields (age, gender, region): INVERTED or BITMAP
<strong>Reason</strong>: Scalar indexes accelerate filtering, vector indexes accelerate similarity search</li></ul><h3 id=scenario-6-beginner-rapid-prototyping>Scenario 6: Beginner Rapid Prototyping</h3><p><strong>Data scale</strong>: Uncertain
<strong>Memory</strong>: Uncertain
<strong>Requirements</strong>: Quick launch, optimize later
<strong>Recommended index</strong>: AUTOINDEX
<strong>Reason</strong>: Let system auto-select, reduce decision cost</p><h2 id=memory-usage-estimation-examples-1-million-128-dimensional-vectors>Memory Usage Estimation Examples (1 million 128-dimensional vectors)</h2><h3 id=ivf-series>IVF Series</h3><ul><li>IVF_PQ (no refinement): ~11 MB</li><li>IVF_PQ + 10% raw refinement: ~62 MB</li><li>IVF_SQ8 (no refinement): ~131 MB</li><li>IVF_FLAT (all raw vectors): ~515 MB</li></ul><h3 id=hnsw-series>HNSW Series</h3><ul><li>HNSW (raw vectors): ~640 MB</li><li>HNSW_PQ (8 bytes/vector): ~136 MB</li></ul><p>Refinement overhead is typically small (e.g. topK=10, expansion rate=5 about 25 KB).</p><h2 id=metric-types-detailed-explanation>Metric Types Detailed Explanation</h2><h3 id=what-are-metric-types>What are Metric Types?</h3><p><strong>Metric types</strong> are mathematical methods used to measure similarity between vectors. Choosing appropriate metric types significantly impacts classification and clustering performance.</p><p><strong>Analogy understanding</strong>: Like measuring distance can use &ldquo;straight-line distance&rdquo;, &ldquo;walking distance&rdquo;, &ldquo;driving distance&rdquo;, vector similarity also has different calculation methods, different methods suit different scenarios.</p><h3 id=supported-metric-types-mapping-table>Supported Metric Types Mapping Table</h3><div class=table-wrapper><table><thead><tr><th>Field Type</th><th>Dimension Range</th><th>Supported Metric Types</th><th>Default Metric Type</th></tr></thead><tbody><tr><td>FLOAT_VECTOR</td><td>2-32,768</td><td>COSINE, L2, IP</td><td>COSINE</td></tr><tr><td>FLOAT16_VECTOR</td><td>2-32,768</td><td>COSINE, L2, IP</td><td>COSINE</td></tr><tr><td>BFLOAT16_VECTOR</td><td>2-32,768</td><td>COSINE, L2, IP</td><td>COSINE</td></tr><tr><td>INT8_VECTOR</td><td>2-32,768</td><td>COSINE, L2, IP</td><td>COSINE</td></tr><tr><td>SPARSE_FLOAT_VECTOR</td><td>No dimension specified</td><td>IP, BM25 (text search only)</td><td>IP</td></tr><tr><td>BINARY_VECTOR</td><td>8-32,768*8</td><td>HAMMING, JACCARD, MHJACCARD</td><td>HAMMING</td></tr></tbody></table></div><p><strong>Notes</strong>:</p><ul><li>BINARY_VECTOR dimensions must be multiples of 8</li><li>BM25 only for SPARSE_FLOAT_VECTOR full-text search scenarios</li></ul><h3 id=metric-types-feature-comparison>Metric Types Feature Comparison</h3><div class=table-wrapper><table><thead><tr><th>Metric Type</th><th>Similarity Characteristic</th><th>Value Range</th><th>Description</th></tr></thead><tbody><tr><td>L2</td><td>Smaller values more similar</td><td>[0, ‚àû)</td><td>Euclidean distance</td></tr><tr><td>IP</td><td>Larger values more similar</td><td>[-1, 1]</td><td>Inner product</td></tr><tr><td>COSINE</td><td>Larger values more similar</td><td>[-1, 1]</td><td>Cosine similarity</td></tr><tr><td>JACCARD</td><td>Smaller values more similar</td><td>[0, 1]</td><td>Jaccard distance</td></tr><tr><td>MHJACCARD</td><td>Smaller values more similar</td><td>[0, 1]</td><td>MinHash Jaccard estimate</td></tr><tr><td>HAMMING</td><td>Smaller values more similar</td><td>[0, dim(vector)]</td><td>Hamming distance</td></tr><tr><td>BM25</td><td>Higher scores more relevant</td><td>[0, ‚àû)</td><td>Text relevance scoring</td></tr></tbody></table></div><h3 id=1-euclidean-distance-l2>1. Euclidean Distance (L2)</h3><p><strong>Definition</strong>: Measures straight-line distance between two points.</p><p><strong>Formula</strong>:</p><pre tabindex=0><code>d(a, b) = ‚àö(Œ£(ai - bi)¬≤)
</code></pre><p>Where a = (a0, a1, &mldr;, an-1) and b = (b0, b1, &mldr;, bn-1) are two points in n-dimensional space.</p><p><strong>Characteristics</strong>:</p><ul><li>Most commonly used distance metric</li><li>Suitable for continuous data</li><li>Milvus doesn&rsquo;t calculate square root in actual computation (performance boost)</li></ul><p><strong>Applicable scenarios</strong>:</p><ul><li>Image similarity search (face recognition)</li><li>General vector search</li><li>Scenarios with uniform data distribution</li></ul><p><strong>Real examples</strong>:</p><ul><li>Two face photos&rsquo; feature vectors, smaller L2 distance means more similar people</li><li>Product feature vectors, smaller L2 distance means closer attributes</li></ul><h3 id=2-inner-product-ip>2. Inner Product (IP)</h3><p><strong>Definition</strong>: Inner product value of two vectors.</p><p><strong>Formula</strong>:</p><pre tabindex=0><code>IP(a, b) = Œ£(ai √ó bi)
</code></pre><p><strong>Characteristics</strong>:</p><ul><li>Larger values indicate more similarity</li><li>Considers both magnitude and angle of vectors</li><li>Suitable for non-normalized data</li></ul><p><strong>Important note</strong>:</p><ul><li>If using IP to calculate similarity, <strong>must normalize vectors first</strong></li><li>After normalization, IP equals cosine similarity</li></ul><p><strong>Applicable scenarios</strong>:</p><ul><li>Recommendation systems (user-item rating prediction)</li><li>Scenarios where vector magnitude matters</li><li>Already normalized vector data</li></ul><p><strong>Real examples</strong>:</p><ul><li>Movie rating prediction, larger IP values mean user likes the movie more</li><li>Document relevance ranking</li></ul><h3 id=3-cosine-similarity>3. Cosine Similarity</h3><p><strong>Definition</strong>: Uses cosine of angle between two vectors to measure similarity.</p><p><strong>Formula</strong>:</p><pre tabindex=0><code>cosine(a, b) = (Œ£(ai √ó bi)) / (‚àö(Œ£ai¬≤) √ó ‚àö(Œ£bi¬≤))
</code></pre><p><strong>Characteristics</strong>:</p><ul><li>Value range [-1, 1]</li><li>Focuses only on direction, not magnitude</li><li>1 means identical direction, 0 means orthogonal, -1 means completely opposite</li></ul><p><strong>Applicable scenarios</strong> (most common):</p><ul><li>Text similarity (NLP)</li><li>Semantic search</li><li>Recommendation systems</li><li>Any scenario ignoring vector magnitude</li></ul><p><strong>Real examples</strong>:</p><ul><li>Two articles&rsquo; semantic similarity, larger COSINE values mean closer topics</li><li>User interest vector matching, high COSINE values mean similar interests</li><li>OpenAI/Cohere models&rsquo; embedding defaults to COSINE</li></ul><h3 id=4-jaccard-distance>4. Jaccard Distance</h3><p><strong>Definition</strong>: Measures similarity between two sets, equal to intersection size divided by union size.</p><p><strong>Formula</strong>:</p><pre tabindex=0><code>JACCARD similarity = |A ‚à© B| / |A ‚à™ B|
JACCARD distance = 1 - JACCARD similarity
</code></pre><p><strong>Characteristics</strong>:</p><ul><li>Only suitable for finite sample sets</li><li>Value range [0, 1]</li><li>For binary variables, equivalent to Tanimoto coefficient</li></ul><p><strong>Applicable scenarios</strong>:</p><ul><li>Binary vectors (BINARY_VECTOR)</li><li>Set similarity comparison</li><li>Label matching</li></ul><p><strong>Real examples</strong>:</p><ul><li>User label matching: User A labels {tech, music, travel}, User B labels {tech, movie, travel}, intersection 2, union 4, JACCARD similarity = 2/4 = 0.5</li><li>Document keyword overlap</li></ul><h3 id=5-minhash-jaccard-mhjaccard>5. MinHash Jaccard (MHJACCARD)</h3><p><strong>Definition</strong>: Method using MinHash signatures to quickly estimate Jaccard similarity.</p><p><strong>Characteristics</strong>:</p><ul><li>Much faster than exact JACCARD calculation</li><li>Suitable for large-scale or high-dimensional scenarios</li><li>Distance = 1 - estimated similarity</li></ul><p><strong>Applicable scenarios</strong>:</p><ul><li>Large document collection similarity</li><li>Large-scale user group similarity analysis</li><li>Genome k-mer set comparison</li></ul><p><strong>Real examples</strong>:</p><ul><li>Massive webpage deduplication</li><li>Large-scale user population similarity analysis</li></ul><h3 id=6-hamming-distance>6. Hamming Distance</h3><p><strong>Definition</strong>: Number of different bits in two equal-length binary strings.</p><p><strong>Formula</strong>:</p><pre tabindex=0><code>Example: 11011001 ‚äï 10011101 = 01000100
Contains 2 ones, so HAMMING distance = 2
</code></pre><p><strong>Characteristics</strong>:</p><ul><li>Only suitable for binary data</li><li>Value range [0, dim(vector)]</li><li>Extremely fast computation</li></ul><p><strong>Applicable scenarios</strong>:</p><ul><li>Binary feature vectors (BINARY_VECTOR)</li><li>Image hash matching</li><li>Error detection and correction</li></ul><p><strong>Real examples</strong>:</p><ul><li>Image perceptual hash (pHash) similarity comparison</li><li>Binary fingerprint matching</li></ul><h3 id=7-bm25-similarity>7. BM25 Similarity</h3><p><strong>Definition</strong>: Text relevance scoring method specifically designed for full-text search.</p><p><strong>Core factors</strong>:</p><ol><li><strong>Term Frequency (TF)</strong>: Frequency of term in document</li><li><strong>Inverse Document Frequency (IDF)</strong>: Term importance across corpus</li><li><strong>Document length normalization</strong>: Avoid long documents getting higher scores</li></ol><p><strong>Formula</strong>:</p><pre tabindex=0><code>score(D, Q) = Œ£ IDF(qi) √ó [TF(qi, D) √ó (k1 + 1)] / [TF(qi, D) + k1 √ó (1 - b + b √ó |D|/avgdl)]
</code></pre><p><strong>Parameter explanation</strong>:</p><ul><li><strong>k1</strong>: Controls TF influence, typical range [1.2, 2.0], Milvus allows [0, 3]</li><li><strong>b</strong>: Controls length normalization degree, range [0, 1]<ul><li>b=0: No normalization</li><li>b=1: Full normalization</li></ul></li><li><strong>avgdl</strong>: Average document length in corpus</li></ul><p><strong>Applicable scenarios</strong>:</p><ul><li>Full-text search (SPARSE_FLOAT_VECTOR)</li><li>Document retrieval</li><li>Question-answering systems</li></ul><p><strong>Real examples</strong>:</p><ul><li>Search engine keyword matching</li><li>Knowledge base QA relevance ranking</li></ul><h3 id=how-to-choose-metric-types>How to Choose Metric Types?</h3><div class=table-wrapper><table><thead><tr><th>Application Scenario</th><th>Recommended Metric Type</th><th>Reason</th></tr></thead><tbody><tr><td>Semantic search, NLP</td><td>COSINE</td><td>Only focuses on direction, unaffected by vector magnitude</td></tr><tr><td>Image search, face recognition</td><td>L2</td><td>Intuitive distance metric, suitable for continuous features</td></tr><tr><td>Recommendation systems (normalized vectors)</td><td>COSINE or IP</td><td>Equivalent after normalization</td></tr><tr><td>Recommendation systems (non-normalized)</td><td>IP</td><td>Considers rating magnitude</td></tr><tr><td>Binary feature matching</td><td>HAMMING</td><td>Fast speed, suitable for binary data</td></tr><tr><td>Set similarity</td><td>JACCARD</td><td>Suitable for labels, keyword matching</td></tr><tr><td>Full-text search</td><td>BM25</td><td>Specifically designed for text retrieval</td></tr><tr><td>Large-scale set matching</td><td>MHJACCARD</td><td>Fast estimation, suitable for massive data</td></tr></tbody></table></div><h3 id=usage-examples>Usage Examples</h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># Vector field using different metric types</span>
</span></span><span style=display:flex><span>index_params<span style=color:#f92672>.</span>add_index(
</span></span><span style=display:flex><span>    field_name<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;embedding&#34;</span>,
</span></span><span style=display:flex><span>    index_type<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;HNSW&#34;</span>,
</span></span><span style=display:flex><span>    metric_type<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;COSINE&#34;</span>  <span style=color:#75715e># Recommended for semantic search</span>
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>index_params<span style=color:#f92672>.</span>add_index(
</span></span><span style=display:flex><span>    field_name<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;image_vector&#34;</span>,
</span></span><span style=display:flex><span>    index_type<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;IVF_FLAT&#34;</span>,
</span></span><span style=display:flex><span>    metric_type<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;L2&#34;</span>  <span style=color:#75715e># Recommended for image search</span>
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>index_params<span style=color:#f92672>.</span>add_index(
</span></span><span style=display:flex><span>    field_name<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;binary_features&#34;</span>,
</span></span><span style=display:flex><span>    index_type<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;BIN_FLAT&#34;</span>,
</span></span><span style=display:flex><span>    metric_type<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;HAMMING&#34;</span>  <span style=color:#75715e># Recommended for binary features</span>
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>index_params<span style=color:#f92672>.</span>add_index(
</span></span><span style=display:flex><span>    field_name<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;sparse_vector&#34;</span>,
</span></span><span style=display:flex><span>    index_type<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;SPARSE_INVERTED_INDEX&#34;</span>,
</span></span><span style=display:flex><span>    metric_type<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;BM25&#34;</span>  <span style=color:#75715e># Recommended for full-text search</span>
</span></span><span style=display:flex><span>)
</span></span></code></pre></div><h3 id=performance-comparison>Performance Comparison</h3><div class=table-wrapper><table><thead><tr><th>Metric Type</th><th>Calculation Speed</th><th>Memory Usage</th><th>Precision</th><th>Applicable Data Types</th></tr></thead><tbody><tr><td>HAMMING</td><td>Extremely fast</td><td>Very low</td><td>Exact</td><td>Binary</td></tr><tr><td>L2</td><td>Fast</td><td>Medium</td><td>Exact</td><td>Float</td></tr><tr><td>IP</td><td>Fast</td><td>Medium</td><td>Exact</td><td>Float</td></tr><tr><td>COSINE</td><td>Medium</td><td>Medium</td><td>Exact</td><td>Float</td></tr><tr><td>JACCARD</td><td>Medium</td><td>Low</td><td>Exact</td><td>Binary/Sets</td></tr><tr><td>MHJACCARD</td><td>Fast</td><td>Low</td><td>Approximate</td><td>Binary/Sets</td></tr><tr><td>BM25</td><td>Medium</td><td>Medium</td><td>Exact</td><td>Sparse vectors</td></tr></tbody></table></div><h2 id=summary>Summary</h2><p>Milvus vector indexes adopt a layered architecture (coarse filtering ‚Üí quantization compression ‚Üí refinement for precision improvement), adaptively optimizing the trade-off between accuracy and performance. For practical selection, recommend combining <strong>data scale</strong>, <strong>hardware environment</strong> (memory/disk/GPU), <strong>query patterns</strong> (topK size, filter ratio) and <strong>business recall rate requirements</strong> for experimental tuning rather than one-size-fits-all approach.</p><p><strong>Key decision points</strong>:</p><ol><li><strong>Choose index type</strong>: Based on capacity, recall rate, filter ratio (HNSW/IVF/DiskANN/FLAT)</li><li><strong>Choose metric type</strong>: Based on data characteristics and application scenarios (COSINE/L2/IP/HAMMING/BM25)</li><li><strong>Tune parameters</strong>: Fine-tune index parameters and search parameters based on actual test results</li></ol></section><footer class=article-footer><section class=article-tags><a href=/en/tags/milvus/>Milvus</a>
<a href=/en/tags/index/>Index</a></section></footer></article><aside class=related-content--wrapper><h2 class=section-title>Related content</h2><div class=related-content><div class="flex article-list--tile"><article><a href=/en/stu/milvus-collection-operations/><div class=article-details><h2 class=article-title>Milvus Collection Operations Detailed Guide</h2></div></a></article><article><a href=/en/stu/milvus-stu-schema/><div class=article-details><h2 class=article-title>Milvus Schema Detailed Guide - Field Types and Data Modeling</h2></div></a></article><article><a href=/en/stu/milvus-install-guide/><div class=article-details><h2 class=article-title>Complete Guide to Installing Milvus Standalone Mode and Common Problem Solutions</h2></div></a></article></div></div></aside><script src=https://giscus.app/client.js data-repo=zg-dd/zg-dd.github.io data-repo-id=R_kgDOQwB7tw data-category=Announcements data-category-id=DIC_kwDOQwB7t84C0WGD data-mapping=pathname data-strict=0 data-reactions-enabled=1 data-emit-metadata=0 data-input-position=bottom data-theme=preferred_color_scheme data-lang=zh-CN data-loading crossorigin=anonymous async></script><script>function setGiscusTheme(e){let t=document.querySelector("iframe.giscus-frame");t&&t.contentWindow.postMessage({giscus:{setConfig:{theme:e}}},"https://giscus.app")}(function(){addEventListener("message",t=>{if(event.origin!=="https://giscus.app")return;e()}),window.addEventListener("onColorSchemeChange",e);function e(){setGiscusTheme(document.documentElement.dataset.scheme==="light"?"preferred_color_scheme":"preferred_color_scheme")}})()</script><footer class=site-footer><section class=copyright>&copy;
2026 Â∞èÂ∑∑ Blog</section><section class=powerby>Built with <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a><br>Theme <b><a href=https://github.com/CaiJimmy/hugo-theme-stack target=_blank rel=noopener data-version=3.33.0>Stack</a></b> designed by <a href=https://jimmycai.com target=_blank rel=noopener>Jimmy</a></section></footer><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
</button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU=" crossorigin=anonymous defer></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css crossorigin=anonymous><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css crossorigin=anonymous></main></div><script src=https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z+KMkF24hUW8WePSA9HM=" crossorigin=anonymous></script><script type=text/javascript src=/ts/main.c922af694cc257bf1ecc41c0dd7b0430f9114ec280ccf67cd2c6ad55f5316c4e.js defer></script><script>(function(){const e=document.createElement("link");e.href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap",e.type="text/css",e.rel="stylesheet",document.head.appendChild(e)})()</script></body></html>